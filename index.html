<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junyao Shi</title>

    <meta name="author" content="Junyao Shi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/penn_logo.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Junyao Shi
                </p>
                <p>Hi! My name is Junyao Shi (ÊñΩÈíßËÄÄ). I'm a final-year PhD student in <a href="https://www.cis.upenn.edu/">Computer and Information Science (CIS)</a> 
                  at <a href="https://www.upenn.edu/">University of Pennsylvania</a> <a href="https://www.grasp.upenn.edu/">GRASP Labortory</a>, 
                  advised by <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>. I am also currently a Research Intern at <a href="https://www.skild.ai/">Skild AI</a>.
                </p>
                <p>
                  My research focuses on robot learning, with a particular emphasis on leveraging internet-scale data and foundation models for building general-purpose robots. 
                  <!-- My ultimate goal is to free humans from dangerous or mundane tasks by entrusting them to machines. -->
                  I see human history as a continual process of automation: as productivity improves, labor becomes more specialized and diverse, making room for the growth of modern culture, science, and institutions. Robotics, to me, is the next frontier of that process.</p>
                <p>
                  Previously, I received my B.S. in Computer Science from <a href="https://www.columbia.edu/">Columbia University</a>, 
                  where I worked with <a href="https://www.cs.columbia.edu/~allen/">Peter Allen</a> on brain-signal guided robot learning 
                  and <a href="https://tonydear.com/">Tony Dear</a> on reinforcement learning for snake robot locomotion.
                </p>
                <p>
                  <span style="color:#b35a20;">Feel free to reach out if interested in research discussion / collaboration! </span>
                  <!-- <span style="color:#b35a20;">I am also looking to mentor highly motivated students to work on research projects.  -->
                    <!-- Please send me an email if you are interested.</span> -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:junys@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=XAhGkq8AAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="data/Junyao_Shi_CV_2025_02.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://x.com/JunyaoShi">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/junyaoshi/">LinkedIn</a> &nbsp;/&nbsp;
                  <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/junyaoshi">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/JunyaoShi.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JunyaoShi.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px 16px 16px 16px;width:100%;vertical-align:middle">
                <ul style="list-style-type: none; padding-left: 0;">
                  <li><strong style="margin-right: 10px;">Jan 2026:</strong> <a href="https://x.com/JunyaoShi/status/2010837806328004721">Here</a>'s a sneak peek into what I've been working on during my Skild AI internship: teaching robots to make scrambled eggs autonomously.</li>
                  <li><strong style="margin-right: 10px;">Dec 2025:</strong> I gave an oral presentation on <a href="https://maestro-robot.github.io/">Maestro</a> at NeurIPS 2025 Workshop on <a href="https://space-in-vision-language-embodied-ai.github.io/">SPACE in Vision, Language, and Embodied AI (SpaVLE)</a>.</li>
                  <li><strong style="margin-right: 10px;">Sep 2025:</strong> I presented an oral talk on <a href="https://maestro-robot.github.io/">Maestro</a> at <a href="https://sites.google.com/view/corl-roboarena">CoRL 2025 RoboArena Workshop</a>.</li>
                  <li><strong style="margin-right: 10px;">Sep 2025:</strong> I began my research internship at <a href="https://www.skild.ai/">Skild AI</a> in bay area.</li>
                  <li><strong style="margin-right: 10px;">Jun 2025:</strong> I will be joining <a href="https://www.skild.ai/">Skild AI</a> in bay area as a Research Intern in Sept 2025.</li>
                  <li><strong style="margin-right: 10px;">Jun 2025:</strong> <a href="https://vlmgineer.github.io/release">VLMgineer</a> is accepted to oral at RSS 2025 Workshop on <a href="https://rss-hardware-intelligence.github.io/">Robot Hardware-Aware Intelligence</a>!</li>
                  <li><strong style="margin-right: 10px;">Jun 2025:</strong> <a href="https://zeromimic.github.io/">ZeroMimic</a> won the <span style="color:red;"><strong>Best Paper Award üèÜ</strong></span> 
                    at CVPR 2025 Workshop on <a href="https://robo-3dvlms.github.io/">3D Vision Language Models for Robotic Manipulation: Opportunities and Challenges</a>!</li>
                  <li><strong style="margin-right: 10px;">Jun 2025:</strong> I will be giving spotlight talks at CVPR 2025 Workshops on 
                    <a href="https://robo-3dvlms.github.io/">3D Vision Language Models (VLMs) for Robotic Manipulation: Opportunities and Challenges</a> and "<a href="https://agents-in-interactions.github.io/">Agents in Interactions, from Humans to Robots</a>".</li>
                  <li><strong style="margin-right: 10px;">Jan 2025:</strong> <a href="https://zeromimic.github.io/">ZeroMimic</a> is accepted to ICRA 2025!</li>
                  <li><strong style="margin-right: 10px;">Jan 2025:</strong> I am a teaching assistant for <a href="https://antonilo.github.io/real_world_robot_learning_sp25/">CIS 7000 / ESE 6800: Real-World Robot Learning</a> this spring.</li>
                  <li><strong style="margin-right: 10px;">May 2024:</strong>Presented <a href="https://sites.google.com/view/pocr">POCR</a> at ICRA 2024.</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <!-- <h2>Research</h2> -->
                <h2>Publications</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="maestro_video" style="opacity:1;">
              <img width="100%" src="images/maestro/maestro-mobile.gif" alt="Maestro Demo">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://maestro-robot.github.io/">
          <span class="papertitle">Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots</span>
        </a>
        <br>
        <strong>Junyao Shi</strong>*,
        <a href="https://www.linkedin.com/in/rujia-yang-810415356/">Rujia Yang</a>*,
        <a href="https://beethoven-q.github.io/kaitianchao/">Kaitian Chao</a>*,
        <a href="https://selina22.github.io/">Selina Wan</a>,
        <a href="https://shaoyifei96.github.io/">Yifei Shao</a>,
        <a href="https://jiahuilei.com/">Jiahui Lei</a>,
        <a href="https://scholar.google.com/citations?user=o67NTxYAAAAJ&hl=en">Jianing Qian</a>,
        <a href="https://vlongle.github.io/">Long Le</a>,
        <a href="https://pratikac.github.io/">Pratik Chaudhari</a>,
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,
        <a href="https://alvinwen428.github.io/">Chuan Wen</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        <em>Under Review</em> 
        <br>
        <a href="https://maestro-robot.github.io/">project page</a>
        /
        <a href="https://maestro-robot.github.io/static/Maestro.pdf">paper</a>
        /
        <a href="https://maestro-robot.github.io/static/Supplementary.pdf">supplementary</a>
        /
        <a href="https://maestro-robot.github.io/Interactive_visualization/">interactive tool-use analysis</a>
        <p></p>
        <p>
          We introduce Maestro, a VLM-driven coding agent that autonomously orchestrates perception, planning, control, and learned policy modules into closed-loop, programmatic robot policies. By combining substantial execution autonomy with a broad and extensible tool repertoire, Maestro achieves strong zero-shot generalization on challenging manipulation tasks while remaining interpretable, easily extensible to new embodiments, and capable of autonomous real-world data collection for training fast, robust downstream policies.
          <!-- We introduce Maestro, a VLM-driven coding agent that autonomously orchestrates perception, planning, control, and learned policy modules‚Äîincluding vision-language-action models‚Äîinto closed-loop robot policies. This design enables strong zero-shot performance in challenging manipulation settings, while preserving interpretability and extensibility and supporting autonomous real-world data collection for training fast, robust downstream policies. -->

        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="vlmgineer_video" style="opacity:1;">
              <img width="100%" src="images/vlmgineer/vlmgineer-intro-cut.gif" alt="VLMgineer Demo">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://vlmgineer.github.io/release">
          <span class="papertitle">VLMgineer: Vision Language Models as Robotic Toolsmiths</span>
        </a>
        <br>
        <a href="https://ggao22.github.io/">George Jiayuan Gao*</a>,
        <a href="http://imtianyuli.com/">Tianyu Li*</a>,
        <strong>Junyao Shi</strong>,
        <a href="https://yihanli126.github.io/">Yihan Li</a><sup>‚Ä†</sup>,
        <a href="https://www.grasp.upenn.edu/people/zizhe-zhang/">Zizhe Zhang</a><sup>‚Ä†</sup>,
        <a href="https://nbfigueroa.github.io/">Nadia Figueroa</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        <em>Under Review</em> 
        <br>
        <a href="https://vlmgineer.github.io/release">project page</a>
        /
        <a href="https://arxiv.org/abs/2507.12644">arxiv</a>
        /
        <a href="https://vlmgineer.github.io/static/VLMgineer.pdf">paper</a>
        /
        <a href="https://x.com/JunyaoShi/status/1936529447605170639">twitter</a>
        <p></p>
        <p>
          We introduce VLMgineer, a novel VLM-driven evolutionary framework that automatically co-design tools and actions to solve robotics task.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="p2r_image" style="opacity:1;">
              <img width="100%" src="images/points2reward/p2r_method.jpg" alt="Points2Reward Method">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Points2Reward: Robotic Manipulation Rewards from Just One Video</span>
        </a>
        <br>
        <strong>Junyao Shi</strong>,
        <a href="https://www.linkedin.com/in/joshua-smith-32b165158/">Joshua Smith</a>,
        <a href="https://scholar.google.com/citations?user=o67NTxYAAAAJ&hl">Jianing Qian</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        <a href="https://points2reward.github.io/">project page</a>
        /
        <a href="https://points2reward.github.io/static/Points2Reward.pdf">paper</a>
        <br>
        <em>Under Review</em> 
        <br>
        <p></p>
        <p>
          We propose "Points2Reward" (P2R), which effectively computes dense rewards from a single video. To do this, P2R tracks task-relevant object points in task demonstrations and policy rollouts, and matches them to compare the desired and achieved object trajectories to generate reward scores.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="zeromimic_video" style="opacity:1;">
              <img width="100%" src="images/zeromimic/zeromimic_pull_gif.gif" alt="ZeroMimic Pull Demo">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://zeromimic.github.io/">
          <span class="papertitle">ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos</span>
        </a>
        <br>
        <strong>Junyao Shi</strong>*,
        <a href="https://zhaolebor.github.io/">Zhuolun Zhao</a>*,
        <a href="https://viccccciv.github.io/Tianyouwang.github.io/">Tianyou Wang</a>,
        <a href="https://ianpedroza.github.io/">Ian Pedroza</a><sup>‚Ä†</sup>,
        <a href="https://www.linkedin.com/in/amy-luo-187962199/">Amy Luo</a><sup>‚Ä†</sup>,
        <a href="https://everloom-129.github.io/">Jie Wang</a>,
        <a href="https://jasonma2016.github.io/">Jason Ma</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        <em>ICRA</em>, 2025 
        <br>
        <font color="red"><strong>Best Paper Award üèÜ</strong></font> at CVPR 2025 Workshop on <a href="https://robo-3dvlms.github.io/">3D Vision Language Models for Robotic Manipulation: Opportunities and Challenges</a>
        <br>
        <a href="https://zeromimic.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2503.23877">arxiv</a>
        /
        <a href="https://zeromimic.github.io/static/ZeroMimic.pdf">paper</a>
        /
        <a href="https://youtu.be/nFhNSsR7TEY">video</a>
        /
        <a href="https://github.com/junyaoshi/ZeroMimic">code</a>
        <p></p>
        <p>
          We introduce ZeroMimic, a system that distills robotic manipulation skills from egocentric human web videos for zero-shot deployment in diverse environments with a variety of objects. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="pocr_video" style="opacity:1;">
              <video width="100%" height="100%" muted autoplay loop playsinline>
                <source src="images/pocr/video-pocr.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://sites.google.com/view/pocr">
          <span class="papertitle">Composing Pre-Trained Object-Centric Representations for Robotics From "What" and "Where" Foundation Models
            </span>
        </a>
        <br>
        <strong>Junyao Shi</strong>*,
        <a href="https://scholar.google.com/citations?user=o67NTxYAAAAJ&hl">Jianing Qian</a>*,
        <a href="https://jasonma2016.github.io/">Jason Ma</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        <em>ICRA</em>, 2024 
        <!-- &nbsp;<font color="red"><strong>(Oral Presentation)</strong></font> -->
        <br>
        <a href="https://sites.google.com/view/pocr">project page</a>
        /
        <a href="https://arxiv.org/abs/2404.13474">paper</a>
        /
        <a href="https://youtu.be/nP28ONBXh-I">video</a>
        /
        <a href="https://github.com/junyaoshi/POCR">code</a>
        <p></p>
        <p>
          POCR chains pre-trained "what" and "where" foundation models to create object-centric representations for robotics. 
          The "where" model identifies object candidates with segmentation masks, which are then bound to slots and encoded by the "what" model, 
          enabling robots to learn policies over these representations.
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="dontyell_image" style="opacity:1;">
              <img src="images/dont_yell/dont_yell.png" width="100%" alt="Don't Yell at Your Robot" style="margin-top: 20px;">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://sites.google.com/sas.upenn.edu/dontyellatyourrobot">
          <span class="papertitle">
            Don't Yell at Your Robot: Physical Correction as the Collaborative Interface for Language Model Powered Robots
          </span>
        </a>
        <br>
        <a href="https://zhangchuye.github.io/">Chuye Zhang</a>*,
        <a href="https://scholar.google.com/citations?user=MpdsJF0AAAAJ&hl=en">Yifei Simon Shao</a>*,
        <a href="https://www.linkedin.com/in/theharshilparekh/">Harshil Parekh</a>,
        <strong>Junyao Shi</strong>,
        <a href="https://pratikac.github.io/">Pratik Chaudhari</a>,
        <a href="https://www.kumarrobotics.org/">Vijay Kumar</a>,
        <a href="https://nbfigueroa.github.io/">Nadia Figueroa</a>
        <br>
        <em>RSS Genarative Modeling meets HRI Workshop</em>, 2024
        <br>
        <!-- <p></p> -->

        <a href="https://sites.google.com/sas.upenn.edu/dontyellatyourrobot">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.12602">paper</a>
        /
        <a href="https://youtu.be/fY_BDzoNCw8?si=Wv90SmtfRmTtrI12">video</a>

      </td>
    </tr>
            
    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="maximizing_bci_image" style="opacity:1;">
              <img src="images/active_brain_rl/active_brain_rl.png" width="100%" alt="Maximizing BCI Human Feedback Using Active Learning" style="margin-top: 20px;">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2008.04873">
          <span class="papertitle">
            Maximizing BCI Human Feedback Using Active Learning
          </span>
        </a>
        <br>
        <a href="https://wangzizhao.github.io/">Zizhao Wang</a>*, 
        <strong>Junyao Shi</strong>*, 
        <a href="https://research.nvidia.com/person/iretiayo-akinola">Iretiayo Akinola</a>*, 
        <a href="https://www.cs.columbia.edu/~allen/">Peter Allen</a>
        <br>
        <em>IROS</em>, 2020   	
        <br>
        <a href="https://arxiv.org/abs/2008.04873">paper</a>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="deep_reinforcement_learning_image" style="opacity:1;">
              <img src="images/snake_rl/snake_rl.png" width="100%" alt="Deep Reinforcement Learning for Snake Robot Locomotion" style="margin-top: 20px;">
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ifatwww.et.uni-magdeburg.de/ifac2020/media/pdfs/3952.pdf">
          <span class="papertitle">
            Deep Reinforcement Learning for Snake Robot Locomotion
          </span>
        </a>
        <br>
        <strong>Junyao Shi</strong>, 
        <a href="https://tonydear.com/">Tony Dear</a>, 
        <a href="http://scottdavidkelly.wikidot.com/">Scott David Kelly</a>
        <br>
        <em>IFAC</em>, 2020
        <br>
        <a href="https://ifatwww.et.uni-magdeburg.de/ifac2020/media/pdfs/3952.pdf">paper</a>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="display:flex; align-items:center; justify-content:center;">
          <div class="one">
            <div class="two" id="accelerated_robot_learning_video" style="opacity:1;">
              <video width="100%" height="100%" muted autoplay loop playsinline>
                <source src="images/brain_rl/icra2020_bci.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="http://crlab.cs.columbia.edu/brain_guided_rl/">
          <span class="papertitle">
            Accelerated Robot Learning via Human Brain Signals
          </span>
        </a>
        <br>
        <a href="https://research.nvidia.com/person/iretiayo-akinola/">Iretiayo Akinola</a>*, 
        <a href="https://wangzizhao.github.io/">Zizhao Wang</a>*, 
        <strong>Junyao Shi</strong>, 
        Xiaomin He, 
        <a href="https://www.linkedin.com/in/pawan-lapborisuth-263971149/">Pawan Lapborisuth</a>, 
        <a href="https://jxu.ai/">Jingxi Xu</a>, 
        <a href="https://davidjosephwatkins.com/">David Watkins-Valls</a>, 
        <a href="https://www.bme.columbia.edu/paul-sajda">Paul Sajda</a>, 
        <a href="https://www.cs.columbia.edu/~allen/">Peter Allen</a>
        <br>
        <em>ICRA</em>, 2020   					   
        <br>
        <a href="http://crlab.cs.columbia.edu/brain_guided_rl/">project page</a>
        /
        <a href="https://arxiv.org/abs/1910.00682">paper</a>
        /
        <a href="https://youtu.be/osJhN0-mF6k?si=5EXQCb-lI9RTCyQP">video</a>
      </td>
    </tr>
            

       

        



          
					<!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a>
								<br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023</a>
								<br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a>
								<br>
								<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a>
								<br>
								<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr> -->
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Teaching</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px 16px 16px 16px;width:100%;vertical-align:middle">
                <ul style="list-style-type: none; padding-left: 0;">
                  <li>Teaching Assistant, <a href="https://antonilo.github.io/real_world_robot_learning_sp25/">Real-World Robot Learning</a>, University of Pennsylvania</li>
                  <li>Teaching Assistant, <a href="https://tonydear.com/teaching/coms4701">Artificial Intelligence</a>, Columbia University</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Mentoring</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px 16px 16px 16px;width:100%;vertical-align:middle">
                <ul style="list-style-type: none; padding-left: 0;">
                  <li><a href="https://zhaolebor.github.io/">Alan Zhuolun Zhao</a> (Penn ROBO MS &rarr; <a href="https://www.skild.ai/">Skild AI</a>)</li>
                  <li><a href="https://viccccciv.github.io/Tianyouwang.github.io/">Tianyou Wang</a> (Penn ROBO MS &rarr; PhD at Oxford)</li>
                  <li><a href="https://www.linkedin.com/in/joshua-smith-32b165158/">Joshua Smith</a> (Penn ROBO MS &rarr; <a href="https://www.skild.ai/">Skild AI</a>)</li>
                  <li><a href="https://www.linkedin.com/in/chenxi-oliver-dong/">Chenxi (Oliver) Dong</a> (Penn CIS MS &rarr; TikTok)</li>
                  <li><a href="https://ggao22.github.io/">George Jiayuan Gao</a> (Penn ROBO MS &rarr; <a href="https://www.dyna.co/">Dyna Robotics</a>)</li>
                  <li><a href="https://ianpedroza.github.io/">Ian Pedroza</a> (Penn ROBO MS &rarr; <a href="https://www.dyna.co/">Dyna Robotics</a>)</li>
                  <li><a href="https://www.linkedin.com/in/amy-luo-187962199/">Amy Luo</a> (Penn ROBO MS)</li>
                  <li>Rujia Yang (Tsinghua Undergrad)</li>
                  <li><a href="https://www.grasp.upenn.edu/people/kaitian-chao/">Kaitian Chao</a> (Penn ROBO MS)</li>
                  <li><a href="https://selina22.github.io/">Selina Wan</a> (Penn ROBO MS)</li>
                  <li><a href="https://anhquanpham.github.io/">Anh-Quan Pham</a> (Penn ROBO MS)</li>
                  <li><a href="https://www.linkedin.com/in/luyang-hu/">Luyang Hu</a> (Penn ROBO MS)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Professional Services</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px 16px 16px 16px;width:100%;vertical-align:middle">
                <ul style="list-style-type: none; padding-left: 0;">
                  <li><strong>Reviewer:</strong> RSS, IROS, ICRA, CoRL, TPAMI, ICLR, ICCV, ECCV</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Other Things</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px 16px 16px 16px;width:100%;vertical-align:middle">
                <p>
                  In my free time, I perform as a saxophonist with <a href="https://www.instagram.com/the_protagonists_band">The Protagonist Band</a> at UPenn.<br>
                  I enjoy skiing, tennis, basketball, and snorkeling.<br>
                  I (try my best to) keep a <a href="https://www.instagram.com/js.travelog">log</a> of my travels. Among my favorite destinations are: Switzerland, Japan, Middle East, Virgin Islands, and Neuschwanstein Castle.<br>
                  My favorite app is <a href="https://www.notion.com/">Notion</a>.<br>
                  I'm also a huge sports fan and proudly root for the Lakers, Manchester United, and FC Barcelona.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <div style="text-align: center;">Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.</div>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
